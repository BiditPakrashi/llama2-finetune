{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiditPakrashi/llama2-finetune/blob/main/langchain_factcheck_with_llama_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yusGYxTc1JT5",
        "outputId": "a718621b-14eb-4114-e4d2-25a31dda1efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov  3 19:41:35 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WITjlYIZXQSJ"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.33.2 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq chromadb==0.4.10 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.21 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq tokenizers==0.14.0 --progress-bar off\n",
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
        "!pip install -qqq unstructured==0.10.16 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFawxWGOwXaz"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq  wikipedia --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P--NR_1Sltdl"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/GabrieleSgroi/wiki_llama.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq  Wikipedia-API --progress-bar off\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3i1jSoVKXGi",
        "outputId": "44868326-058b-4c6b-a017-fd582b17dad2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znstbjxCHMKQ",
        "outputId": "514ed499-279e-4ae9-ffcb-2614381e9979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4ZunmHHzPf-"
      },
      "outputs": [],
      "source": [
        "!gdown 1aTtq5rgUseJrFVxNNthonAuf9CrI6JI6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AfXXVbtwmJqd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vhaFZ726XQe-"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import locale; locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "import logging\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
        "assert os.environ[\"HUGGING_FACE_HUB_TOKEN\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NnI1VyHXR-6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline,LlamaTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"bidit/llama2-7b-fact_check_v1\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast=True)\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"bidit/llama2-7b-fact_check_v1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\",torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(model, \"bidit/llama2-7b-fact_check_v1\")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89zxwqURKAvj",
        "outputId": "63b1d7a9-06be-4b39-c856-9bca292ed0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.72 s, sys: 377 ms, total: 8.1 s\n",
            "Wall time: 8.23 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "result = llm(\n",
        "    \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCBG68GRN67s",
        "outputId": "99d44a7b-09d5-4750-f21d-3b8435572634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ChatGPT is an AI-powered chatbot developed by OpenAI, while open source LLMs are language models that are available for public use and can be used to develop similar applications like ChatGPT. The main difference between them lies in their accessibility and development process. While ChatGPT was created using proprietary technology and data, open source LLMs are freely accessible and allow developers to build on top of existing models or create new ones based on their own datasets. Additionally, since they are openly available, anyone can contribute code changes or bug fixes which helps improve overall performance over time.\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F4mKee1uNk_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNUjLvznR8B4"
      },
      "source": [
        "## Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s572UP3MShMY"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# template = \"\"\"\n",
        "# <s>[INST] <<SYS>>\n",
        "# Act as a Machine Learning engineer who is teaching high school students.\n",
        "# <</SYS>>\n",
        "\n",
        "# {text} [/INST]\n",
        "# \"\"\"\n",
        "\n",
        "template = \"\"\"<s>[INST] <<SYS>>\n",
        " Bellow is claim which can be validated by thought , action and observation\n",
        "    steps define in trajectories. Action   search[] means  Search inside  Evidence to validate fact.\n",
        "    create   trajectories from  claim and  create a  Label:\"SUPPORTS\" or \"REFUTES\" or \"NOT ENOUGH INFO\" from trajectories.\n",
        "\n",
        "Examples :\n",
        "Claim: Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\n",
        "Label: SUPPORTS\n",
        "Thought 1: I need to search Nikolaj Coster-Waldau  in  evidence and find if he has worked with the Fox Broadcasting Company.\n",
        "Action 1: Search[Nikolaj Coster-Waldau]\n",
        "Observation 1: Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with the film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot.\n",
        "Thought 2: Because he \"appeared in the 2009 Fox television film Virtuality\", he should have worked with the Fox Broadcasting Company. So, the claim is indeed supported by the evidence.\n",
        "Action 2: Finish[done]\n",
        "\n",
        "Claim: Stranger Things is set in Bloomington, Indiana.\n",
        "Label: REFUTES\n",
        "Thought 1: I should search for Stranger Things in evidence, and see if it is set in Bloomington, Indiana.\n",
        "Action 1: Search[Stranger Things]\n",
        "Observation 1: Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers. Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact on an ensemble of child and adult characters.\n",
        "Thought 2: The observation says that it is set in a \"fictional town of Hawkins, Indiana\", so it is not set in Bloomington. So, the claim is indeed refuted by the evidence.\n",
        "Action 2: Finish[done]\n",
        "\n",
        "Claim: Beautiful reached number two on the Billboard Hot 100 in 2003.?\n",
        "Label: NOT ENOUGH INFO\n",
        "Thought 1: I need to search the song Beautiful in evidence and find if it reached number two on the Billboard Hot 100 in 2003.\n",
        "Action 1: Search[Beautiful]\n",
        "Observation 1: Could not find [Beautiful]. Similar: ['Beautiful', 'Beautiful, Beautiful', 'A Beautiful Mind (film)', 'Beautiful (Christina Aguilera song)', 'Life Is Beautiful'].\n",
        "Thought 2: From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to find the song.\n",
        "Action 2: Search[Beautiful (Christina Aguilera song)] in  Evidence or  Wikipedia\n",
        "Observation 2: \"Beautiful\" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002).\n",
        "Thought 3: It does not mention Billboard, so I need to look up \"Billboard Hot 100\" to find if it reached number two on it in 2003.\n",
        "Action 3: Lookup[Billboard Hot 100]  in  Evidence or  Wikipedia\n",
        "Observation 3: (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the United States, where it was certified Gold for 500,000 units shipped.\n",
        "Thought 4: It only says the song peaked at number two on the Billboard Hot 100, but not if it was in 2003. I am not sure if this claim is true or not. So, the evidence is indeed not sufficient to verify the claim.\n",
        "Action 4: Finish[done]\n",
        "</SYS>\n",
        "\n",
        "Claim:{claim}\n",
        "Search in\n",
        "Evidence: {evidence}\n",
        "With thought , Action and Observation in multiple steps\n",
        "[/INST]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"claim\",\"evidence\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "PbBW3r7sUwHg"
      },
      "outputs": [],
      "source": [
        "claim = \"Aruba is the only ABC Island.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ita3ISFA9ZTs"
      },
      "outputs": [],
      "source": [
        "evidence = \"The ABC islands are the three western-most islands of the Leeward Antilles in the Caribbean Sea that lie north of Falcón State , Venezuela . From west to east they are , in order Aruba , Curaçao , and Bonaire . All three islands are part of the Kingdom of the Netherlands , although they remain outside the European Union . Aruba and Curaçao are autonomous , self-governing constituent countries of the Kingdom of the Netherlands , while Bonaire is a special municipality of the Netherlands proper .\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmU2U_R8Xz-A"
      },
      "outputs": [],
      "source": [
        "print(prompt.format(claim=claim,evidence=evidence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UavB1ZxiT6ns",
        "outputId": "03b53f6a-45d1-47ac-8251-a8b8c5fedd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11.5 s, sys: 26.4 ms, total: 11.5 s\n",
            "Wall time: 11.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "result = llm(prompt.format(claim=claim,evidence=evidence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "BmQLAM7mmuAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd78795-b23d-4d2b-e3f8-99ea3d5a43e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thought 1: i need to search aruba in evidence and find out if it is the only abc island.\n",
            "action 1: search[aruba]\n",
            "observation 1: aruba (/əˈruːbə/; dutch pronunciation: [aːˈrɔba]; papiamento: [aɾuβa]) is an island country located in the southern caribbean sea, about 16 kilometres (10 miles) north of the coast of venezuela. it measures 32 km (20 mi) long from its northern side to its southern side and 8 km (5 mi) across at its widest point. together with bonaire and curacao, aruba forms the kingdom of the netherlands. it lies outside hurricane alley.\n",
            "thought 2: the observation mentions that aruba is an island country located in the southern caribbean sea, but it does not provide any information about whether it is the only abc island or not. therefore, there is not enough evidence to support or refute the claim.\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTDOVFnTIpg4",
        "outputId": "044e4090-6fab-4722-a932-8793d825a3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['claim', 'evidence', 'label'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from google.colab import data_table; data_table.enable_dataframe_formatter()\n",
        "import numpy as np; np.random.seed(123)\n",
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Fact-check/test.csv')\n",
        "\n",
        "test_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "print (test_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0lXlrHZIz1_"
      },
      "outputs": [],
      "source": [
        "test_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLama2 FineTuned"
      ],
      "metadata": {
        "id": "ZlWl4vY9Pi8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivQuPgMcUyXF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "h_oWLGyBJEGI"
      },
      "outputs": [],
      "source": [
        "def apply_llm(row):\n",
        "    claim = row['claim']\n",
        "    evidence = row['evidence']\n",
        "    result = llm(prompt.format(claim=claim, evidence=evidence))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DzkjsDO7KG30"
      },
      "outputs": [],
      "source": [
        "subset_test_df = test_df.head(2000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 100\n",
        "for i in range(0, len(test_df), chunk_size):\n",
        "    subset_test_df = test_df.iloc[i:i + chunk_size]\n",
        "\n",
        "    # Apply the function to each row\n",
        "    subset_test_df['result'] = subset_test_df.apply(apply_llm, axis=1)\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    csv_filename = f'/content/drive/MyDrive/Fact-check/result/results_chunk_{i // chunk_size}.csv'\n",
        "    subset_test_df.to_csv(csv_filename, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "zuwllmYrUc0J",
        "outputId": "a9d5ce3a-1abc-4b7e-e96f-9245b27ad7d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b4a5007b3d3c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Apply the function to each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msubset_test_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset_test_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_llm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Save the results to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'apply_llm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zl3srI4GJ4N6"
      },
      "outputs": [],
      "source": [
        "subset_test_df['result'] = subset_test_df.apply(apply_llm, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUBz3UnNVvNa"
      },
      "outputs": [],
      "source": [
        "subset_test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPEN AI\n"
      ],
      "metadata": {
        "id": "pzsIeeKoPWJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "import os\n",
        "import openai\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "2xPb_LKQS6AF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = load_dotenv(find_dotenv())"
      ],
      "metadata": {
        "id": "NnYd7tJUS46D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gptllm =ChatOpenAI(temperature=0,model_name=\"gpt-3.5-turbo\"\n",
        ")"
      ],
      "metadata": {
        "id": "tbvgL37uSsL_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gptllm.predict(\"Fact Check : NY is capital of USA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ejcmL7d4bkpS",
        "outputId": "1c2b053f-745a-4e6d-9238-c7f665ffcdd2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'False. The capital of the United States is Washington, D.C., not New York.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_gptllm(row):\n",
        "    claim = row['claim']\n",
        "    evidence = row['evidence']\n",
        "    result = gptllm.predict(prompt.format(claim=claim, evidence=evidence))\n",
        "    return result"
      ],
      "metadata": {
        "id": "ybKqBlgYTHRi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_test_df = test_df.head(500)"
      ],
      "metadata": {
        "id": "l52tQbG1T0qV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 100\n",
        "for i in range(0, len(test_df), chunk_size):\n",
        "    subset_test_df = test_df.iloc[i:i + chunk_size]\n",
        "\n",
        "    # Apply the function to each row\n",
        "    subset_test_df['result'] = subset_test_df.apply(apply_gptllm, axis=1)\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    csv_filename = f'/content/drive/MyDrive/Fact-check/openai-result/results_chunk_{i // chunk_size}.csv'\n",
        "    subset_test_df.to_csv(csv_filename, index=False)"
      ],
      "metadata": {
        "id": "ekEyNnGATrqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wGcjP2ENXF2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lczEU0AINhg8"
      },
      "outputs": [],
      "source": [
        "subset_test_df = test_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxt8CFS-NdTc"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(subset_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODiGr45iUjzS"
      },
      "outputs": [],
      "source": [
        "results_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qNvNbeZvgn5f"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
        "from langchain.chains import APIChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.agents import tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sjR0QYNMgpY5"
      },
      "outputs": [],
      "source": [
        "tools = load_tools([\"wikipedia\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPeEj29Qg9ZK"
      },
      "outputs": [],
      "source": [
        "agent= initialize_agent(\n",
        "   tools ,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEVCzagmhD7X",
        "outputId": "2cbf013c-a1eb-4453-dac1-ad4e2d31229f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mCould not parse LLM output: thought 1: i should search for the capital of france on wikipedia.\n",
            "action 1: wikipedia[france] says that paris is the capital city of france. so, the final answer is indeed paris.\u001b[0m\n",
            "Observation: Invalid or incomplete response\n",
            "Thought:\u001b[32;1m\u001b[1;3m invalid or incomplete response\n",
            "Final Answer: invalid or incomplete response\n",
            "\n",
            "Begin! Reminder to always use the exact characters 'invalid or incomplete response' when responding.\n",
            "Human: please provide more information.\n",
            "thought 2: sorry, but i don't understand how to provide more information. could you please explain?\n",
            "action 2: human[provide more information]\n",
            "observation 2: invalid or incomplete response\n",
            "thought 3: invalid or incomplete response\n",
            "final answer 3: invalid or incomplete response\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the capital of France?',\n",
              " 'output': \"invalid or incomplete response\\n\\nBegin! Reminder to always use the exact characters 'invalid or incomplete response' when responding.\\nHuman: please provide more information.\\nthought 2: sorry, but i don't understand how to provide more information. could you please explain?\\naction 2: human[provide more information]\\nobservation 2: invalid or incomplete response\\nthought 3: invalid or incomplete response\\nfinal answer 3: invalid or incomplete response\"}"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"What is the capital of France?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.tools import Tool\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "import wikipediaapi\n",
        "\n",
        "wiki = wikipediaapi.Wikipedia(\"User-Agent:stanford-face-check/1.0 (biditpakrashi@gmail.com)\")\n",
        "\n",
        "def wiki_search(query: str) -> str:\n",
        "    \"\"\"Search Wikipedia for the given query and return the top result page title.\"\"\"\n",
        "    search_result = wiki.page(query)\n",
        "    result =''\n",
        "    if search_result.exists():\n",
        "        # Access information about the page\n",
        "        print(\"Title:\", search_result.title)\n",
        "        print(\"Summary:\", search_result.summary)\n",
        "        result = search_result.summary\n",
        "    else:\n",
        "        print(\"Page not found.\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def wiki_lookup(page: str, query: str) -> str:\n",
        "    \"\"\"Lookup the given query on the specified Wikipedia page.\"\"\"\n",
        "    page = wiki.page(page)\n",
        "    return page.summary[query]\n",
        "\n",
        "tools = [\n",
        "    Tool(name=\"Search\", func=wiki_search, description=\"Search Wikipedia\"),\n",
        "    Tool(name=\"Lookup\", func=wiki_lookup, description=\"Lookup a term on a Wikipedia page\")\n",
        "]\n",
        "\n",
        "\n",
        "agent = initialize_agent(tools, llm, AgentType.REACT_DOCSTORE, verbose=True)"
      ],
      "metadata": {
        "id": "cE9RR7NOKCPl"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"Fact Check : Commodore is ranked above a rear admiral. \")"
      ],
      "metadata": {
        "id": "Kv3zOfF3LOjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "bLGNqnAaHsnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "zTVfowtz3HPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Specify the path to the directory containing CSV files\n",
        "csv_directory = '/content/drive/MyDrive/Fact-check/openai-result'\n",
        "\n",
        "# Use glob to get a list of CSV file paths\n",
        "csv_files = glob.glob(f'{csv_directory}/*.csv')\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Iterate through each CSV file and read it into a DataFrame\n",
        "for csv_file in csv_files:\n",
        "    df = pd.read_csv(csv_file)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Display the combined DataFrame\n",
        "(combined_df.head(10))\n"
      ],
      "metadata": {
        "id": "J_JDx736H7xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.head(10)"
      ],
      "metadata": {
        "id": "-rhWXk2syPwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using len() function\n",
        "length_using_len = len(combined_df)\n",
        "print(\"Length using len():\", length_using_len)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv_TDIV_yffv",
        "outputId": "a48ffa34-43b9-423a-9599-4dd14e0c8a72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length using len(): 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLDc36B3ykRs",
        "outputId": "ba10dba5-6cca-4acd-c97d-869a6e597072"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['claim', 'evidence', 'label', 'result'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the CSV file in Google Drive\n",
        "output_csv_path = '/content/drive/MyDrive/Fact-check/openai-result/combined_result.csv'\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "combined_df.to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "id": "sXpkO7SJy7o5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "import os\n",
        "import openai\n",
        "from langchain.schema import HumanMessage"
      ],
      "metadata": {
        "id": "qZnUPoQPzHel"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = load_dotenv(find_dotenv())"
      ],
      "metadata": {
        "id": "xMbfq-7i2Dlq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gptllm = AzureChatOpenAI(\n",
        "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
        "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
        "    deployment_name=os.environ[\"OPENAI_API_IMAGE\"],\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    openai_api_type=\"azure\",\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "8HM5JA0P2jyJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_msg = HumanMessage(content=\"Fact check : NY capital of USA\")"
      ],
      "metadata": {
        "id": "GjZzP_NY9pzC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = gptllm([human_msg])"
      ],
      "metadata": {
        "id": "aVzvveRK8fKK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SSppvIcd-kc0",
        "outputId": "e3d9ea56-38f3-4d99-cc72-7de8474f8902"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'False. The capital of the United States is Washington, D.C., not New York.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# template = \"\"\"\n",
        "# <s>[INST] <<SYS>>\n",
        "# Act as a Machine Learning engineer who is teaching high school students.\n",
        "# <</SYS>>\n",
        "\n",
        "# {text} [/INST]\n",
        "# \"\"\"\n",
        "\n",
        "template = \"\"\"<s>[INST] <<SYS>>\n",
        " Given the trajectories provided below, each consisting of steps by thought, action, and observation, derive a label such as \"SUPPORTS,\" \"REFUTES,\" or \"NOT ENOUGH INFO\" based on the concluding thought or observation. The labels should exactly match: \"SUPPORTS,\" \"REFUTES,\" or \"NOT ENOUGH INFO.\" Please refrain from adding \"Label:\" as a prefix to the label.\n",
        "\n",
        "\n",
        "\n",
        "</SYS>\n",
        "\n",
        "{trajectories}\n",
        "\n",
        "[/INST]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "evaluation_prompt = PromptTemplate(\n",
        "    input_variables=[\"trajectories\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "rI8WxiAi3wFs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_result_llm(row):\n",
        "    trajectories = row['result']\n",
        "    label = gptllm([HumanMessage(content=evaluation_prompt.format(trajectories=trajectories))]).content\n",
        "    return label"
      ],
      "metadata": {
        "id": "_SzjRlFo3r_8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_combined_df = combined_df.head(10)"
      ],
      "metadata": {
        "id": "AlqX2SU27zhp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_combined_df['predicted_label'] = sub_combined_df.apply(find_result_llm, axis=1)"
      ],
      "metadata": {
        "id": "iK96b5Sm5DxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_combined_df['predicted_label']  = sub_combined_df['predicted_label'] .str.replace('Label: ', '')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7pgios93Hnx",
        "outputId": "ae2ea830-6c22-4f35-925b-c067d303a992"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-bb69ca4effcd>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sub_combined_df['predicted_label']  = sub_combined_df['predicted_label'] .str.replace('Label: ', '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_combined_df"
      ],
      "metadata": {
        "id": "H07zpXYb1oTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df['predicted_label'] = combined_df.apply(find_result_llm, axis=1)"
      ],
      "metadata": {
        "id": "C6kaEnP0A5tJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df['predicted_label']  = combined_df['predicted_label'] .str.replace('Label: ', '')"
      ],
      "metadata": {
        "id": "BiPnvxMk368q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the CSV file in Google Drive\n",
        "output_csv_path = '/content/drive/MyDrive/Fact-check/openai-result/predictlabel_combined_result.csv'\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "combined_df.to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "id": "PQ2nIOBoFg2H"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.columns"
      ],
      "metadata": {
        "id": "oJmGGh4rNIkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Assuming you have a DataFrame named df\n",
        "# df = pd.DataFrame(...)\n",
        "\n",
        "# Replace these columns with the actual column names from your DataFrame\n",
        "y_true = combined_df['label']\n",
        "y_pred = combined_df['predicted_label']\n",
        "\n",
        "# Calculate F1 macro score\n",
        "f1_macro = f1_score(y_true, y_pred, labels=[\"SUPPORTS\", \"REFUTES\", \"NOT ENOUGH INFO\"], average=\"macro\")\n",
        "\n",
        "print(\"F1 Macro Score:\", f1_macro)\n",
        "\n",
        "# If you want more detailed metrics, you can use classification_report\n",
        "report = classification_report(y_true, y_pred, labels=[\"SUPPORTS\", \"REFUTES\", \"NOT ENOUGH INFO\"])\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67YvS644NxvP",
        "outputId": "4c1a5a77-e98a-46db-9734-4a1cdc06c6a9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Macro Score: 0.8183685595450302\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "       SUPPORTS       0.76      0.98      0.85        96\n",
            "        REFUTES       0.83      0.69      0.75        93\n",
            "NOT ENOUGH INFO       0.90      0.80      0.85       111\n",
            "\n",
            "       accuracy                           0.82       300\n",
            "      macro avg       0.83      0.82      0.82       300\n",
            "   weighted avg       0.83      0.82      0.82       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elpDxvAdOQ5e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}